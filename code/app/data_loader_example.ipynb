{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is example code that demonstrates use of our custom data loader for the POCOVID-Net training data. Much of this code is based on snippets from the the official PyTorch tutorial on [custom datasets, dataloaders, and transforms](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclass `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PocovidDataset(Dataset):\n",
    "  \"\"\"Subclass of Dataset for POCOVID-Net data\"\"\"\n",
    "  \n",
    "  def __init__(self, root_dir, transform=None):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "          root_dir (string): Directory with all the images.\n",
    "          transform (callable, optional): Optional transform to be applied\n",
    "              on a sample.\n",
    "      \"\"\"\n",
    "      self.root_dir = root_dir\n",
    "      self.transform = transform\n",
    "      self.covid_dir = root_dir + '/' + 'covid'\n",
    "      self.pneu_dir = root_dir + '/' + 'pneumonia'\n",
    "      self.regular_dir = root_dir + '/' + 'regular'\n",
    "      \n",
    "      self.covid_class = 0\n",
    "      self.pneu_class = 1\n",
    "      self.regular_class = 2\n",
    "\n",
    "      # Modified code snippet from Daniel Stutzbach: https://stackoverflow.com/a/2632251\n",
    "      dir_items = lambda d: [d + \"/\" + name for name in os.listdir(d) if os.path.isfile(d + \"/\" + name)]\n",
    "\n",
    "      covid_items = dir_items(self.covid_dir)\n",
    "      pneu_items = dir_items(self.pneu_dir)\n",
    "      regular_items = dir_items(self.regular_dir)\n",
    "      \n",
    "      num_covid = len(covid_items)\n",
    "      num_pneu = len(pneu_items)\n",
    "      num_regular = len(regular_items)\n",
    "      \n",
    "      self.img_info = []\n",
    "      for covid_filename in covid_items:\n",
    "        self.img_info.append((covid_filename,self.covid_class))\n",
    "      for pneu_filename in pneu_items:\n",
    "        self.img_info.append((pneu_filename,self.pneu_class))\n",
    "      for regular_filename in regular_items:\n",
    "        self.img_info.append((regular_filename,self.regular_class)) \n",
    "\n",
    "      self.transform = transform\n",
    "      self.num_images = num_covid + num_pneu + num_regular\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.num_images\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    img_name, img_class = self.img_info[idx]\n",
    "    image = io.imread(img_name)\n",
    "    sample = {'image': image, 'class': img_class} \n",
    "    \n",
    "    if self.transform:\n",
    "      sample = self.transform(sample)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations\n",
    "These are the same or very similar to transformations defined in the official PyTorch tutorial on [custom datasets, dataloaders, and transforms](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy Rescale from the tutorial, and create a \"rotate\" tranform and a \"translate\" transform\n",
    "# because the paper augments data in place by rotating up to 10 degrees, and translating up to 10 \n",
    "# degrees\n",
    "\n",
    "# Modified from official PyTorch tutorial: \n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, img_cls = sample['image'], sample['class']\n",
    "\n",
    "        # The authors do a \"naive\" resize to 224x224 and do not attempt to preserve aspect ratio,\n",
    "        # see line 88 of evaluate_covid19.py on the \"arxiv\" tag: \n",
    "        # https://github.com/jannisborn/covid19_pocus_ultrasound/blob/arxiv/pocovidnet/pocovidnet/evaluate_covid19.py#L88\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'class': img_cls}\n",
    "\n",
    "# From official PyTorch tutorial: \n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, img_cls = sample['image'], sample['class']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'class': img_cls}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of `PocovidDataset`, applying transformations \"in place\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this example, I am assuming this notebook is being run from the `code/app` directory in this repository. If you use a different working directory, you will need to adjust the `root_dir` argument to the `PocovidDataset` constructor accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nathan/Code/covid19_pocus_ultrasound_pytorch/code/app'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() # get current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = PocovidDataset(root_dir='../data/pocus_images',\n",
    "                                     transform=transforms.Compose([\n",
    "                                       Rescale((224,224)),\n",
    "                                       ToTensor()\n",
    "                                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/pocus_images/covid/Cov-Patchy B lines with Sparing.mp4_frame28.jpg',\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset.img_info[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Using a `for` loop\n",
    "First, let's iterate using a simple `for` loop. This works, but may not be as powerful / as efficient as using a `DataLoader` (see Option B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 224, 224]) 0\n",
      "1 torch.Size([3, 224, 224]) 0\n",
      "2 torch.Size([3, 224, 224]) 0\n",
      "3 torch.Size([3, 224, 224]) 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].size(), sample['class'])\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Using `torch.utils.data.DataLoader`\n",
    "**TODO:** get the commented code below to actually work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: 0, 1, and 2 are hardcoded both here and where we set self.covid_class, self.pneu_class, and \n",
    "# self.regular_class in the __init__ function for PocovidDataset\n",
    "lbl_map = {0:\"COVID\",1:\"Pneumonia\",2:\"Regular\"} \n",
    "\n",
    "# # based on show_landmarks_batch in the official PyTorch dataloader tutorial\n",
    "def show_labeled_images_batch(sample_batched):\n",
    "  \"\"\"Show image with class labels for a batch of samples.\"\"\"\n",
    "  images_batch, classes_batch = sample_batched['image'], sample_batched['class']\n",
    "  print(images_batch)\n",
    "  print(classes_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 3, 224, 224]) tensor([0, 0, 1, 0])\n",
      "1 torch.Size([4, 3, 224, 224]) tensor([0, 1, 1, 1])\n",
      "2 torch.Size([4, 3, 224, 224]) tensor([0, 0, 1, 0])\n",
      "3 torch.Size([4, 3, 224, 224]) tensor([1, 0, 0, 0])\n",
      "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.5707e-01, 1.5558e-01, 1.4091e-01,  ..., 1.0896e-01,\n",
      "           1.0352e-01, 1.0297e-01],\n",
      "          [1.6240e-01, 1.6024e-01, 1.3905e-01,  ..., 1.0146e-01,\n",
      "           9.8192e-02, 9.7860e-02],\n",
      "          [1.6294e-01, 1.6072e-01, 1.3886e-01,  ..., 1.0069e-01,\n",
      "           9.7650e-02, 9.7340e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.4923e-01, 1.4773e-01, 1.3307e-01,  ..., 1.0112e-01,\n",
      "           9.5679e-02, 9.5126e-02],\n",
      "          [1.5456e-01, 1.5240e-01, 1.3121e-01,  ..., 9.3614e-02,\n",
      "           9.0349e-02, 9.0017e-02],\n",
      "          [1.5510e-01, 1.5288e-01, 1.3102e-01,  ..., 9.2850e-02,\n",
      "           8.9807e-02, 8.9497e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.6099e-01, 1.5950e-01, 1.4484e-01,  ..., 1.1289e-01,\n",
      "           1.0744e-01, 1.0689e-01],\n",
      "          [1.6632e-01, 1.6417e-01, 1.4297e-01,  ..., 1.0538e-01,\n",
      "           1.0211e-01, 1.0178e-01],\n",
      "          [1.6686e-01, 1.6464e-01, 1.4278e-01,  ..., 1.0461e-01,\n",
      "           1.0157e-01, 1.0126e-01]]],\n",
      "\n",
      "\n",
      "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[2.9228e-02, 2.3502e-02, 1.0767e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.6654e-02, 2.1772e-02, 1.0767e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.0711e-02, 1.7560e-02, 1.0158e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [2.3099e-02, 1.9165e-02, 1.0767e-02,  ..., 7.2343e-03,\n",
      "           5.3571e-03, 4.3067e-03],\n",
      "          [2.0273e-02, 1.6071e-02, 7.1254e-03,  ..., 7.3279e-03,\n",
      "           5.6898e-03, 4.6394e-03],\n",
      "          [1.8172e-02, 1.3971e-02, 5.0245e-03,  ..., 7.2388e-03,\n",
      "           5.6898e-03, 4.6394e-03]],\n",
      "\n",
      "         [[1.3138e-04, 3.2365e-04, 7.1779e-04,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.2365e-04, 7.9727e-04, 1.7682e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.5690e-04, 1.6182e-03, 3.6171e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.5317e-02, 1.1472e-02, 3.8934e-03,  ..., 7.2343e-03,\n",
      "           5.3571e-03, 4.3067e-03],\n",
      "          [1.2824e-02, 9.1992e-03, 1.9182e-03,  ..., 7.3279e-03,\n",
      "           5.6898e-03, 4.6394e-03],\n",
      "          [1.0916e-02, 7.5720e-03, 7.7867e-04,  ..., 7.2388e-03,\n",
      "           5.6898e-03, 4.6394e-03]],\n",
      "\n",
      "         [[2.6276e-04, 6.4729e-04, 5.1836e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.4729e-04, 1.5945e-03, 6.9281e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.7073e-03, 3.7191e-03, 1.0335e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [2.7021e-02, 2.3087e-02, 1.4356e-02,  ..., 7.2343e-03,\n",
      "           5.3571e-03, 4.3067e-03],\n",
      "          [2.4195e-02, 1.9993e-02, 1.0714e-02,  ..., 7.3279e-03,\n",
      "           5.6898e-03, 4.6394e-03],\n",
      "          [2.2094e-02, 1.7892e-02, 8.6134e-03,  ..., 7.2388e-03,\n",
      "           5.6898e-03, 4.6394e-03]]],\n",
      "\n",
      "\n",
      "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]]]], dtype=torch.float64)\n",
      "tensor([1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['class'])\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        #plt.figure()\n",
    "        show_labeled_images_batch(sample_batched)\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
